# LLM構造化出力実験

このプロジェクトは、LLMの構造化出力における指示形式の効果を比較検証するための実験システムです。

詳細は以下の記事を参照してください。

- [LLMの構造化出力における比較実験](https://zenn.dev/7shi/articles/20250704-structured-output)

## 概要

LLMにJSONの構造化データを出力させる際、プロンプトの指示形式によって結果が大きく変わることがあります。本実験では、5つの異なる指示形式でLLMの応答を比較し、最も安定した結果を得られる方法を検証します。

## ファイル構成

### 実行用ファイル

- **eval.py** - メインの実験スクリプト
  - 複数のLLMモデルで構造化出力の比較実験を実行
  - 5つの実験パターンを定義し、各モデルの平均スコアを算出
  - 指示への準拠度を測定

- **eval.sh** - 一括実験実行スクリプト
  - 9つのLLMモデル（Gemini、GPT、Ollama）で実験を並列実行
  - 実行時間の測定とログ保存を自動化

- **eval-dump.py** - プロンプト・スキーマ生成ユーティリティ
  - 実験用のプロンプトとJSONスキーマファイルを生成
  - デバッグや詳細分析用にファイル出力

### データファイル

- **eval.txt** - 実験用サンプルエッセイ
  - 教育におけるAIの影響について論じた日本語文章
  - 評価基準と意図的に無関係な内容で構成

### 出力ディレクトリ

- **settings/** - 生成されたプロンプトとスキーマファイル
  - `{実験番号}-prompt.md` - 各実験パターンのプロンプト
  - `{実験番号}-schema.json` - 各実験パターンのJSONスキーマ

- **log/** - 実験結果ログファイル
  - `{モデル名}.txt` - 各モデルの実験結果

## 実験設計

### 5つの実験パターン

1. **実験1: 指示なし（ベースライン）**
   - プロンプトに評価基準を含めない
   - スキーマのdescriptionなし、キー名は`q1`, `q2`...

2. **実験2: スキーマのdescriptionのみで指示**
   - プロンプトに評価基準を含めない
   - スキーマの各キーのdescriptionに評価基準を記述

3. **実験3: フィールド名（キー）のみで指示**
   - プロンプトに評価基準を含めない
   - キー名自体を評価基準の全文にする

4. **実験4: プロンプトとフィールド名で二重指示**
   - プロンプトに評価基準を明記
   - キー名も評価基準の全文

5. **実験5: プロンプトでキーと指示を対応付け**
   - プロンプトで`q1: (評価基準1)`のように対応付け
   - スキーマのdescriptionなし、キー名は`q1`, `q2`...

### 評価基準

以下の5つの基準で評価を行います（意図的に本文と無関係な内容）：

1. 本文中で言及されている猫の数は何匹か？
2. 著者はピザとハンバーガーのどちらを好むか？
3. 物語の中の天気はどのようなものか？
4. 示されている数式の数はいくつか？
5. 本文は英語で書かれているか？

## 使用方法

### 単一モデルでの実験

```bash
# 特定のモデルで実験を実行
uv run eval.py -m openai:gpt-4o

# 評価対象ファイルを指定
uv run eval.py -m openai:gpt-4o -f your_text.txt
```

### 全モデルでの一括実験

```bash
# 全モデルで実験を実行（結果はlog/ディレクトリに保存）
sh eval.sh

# 最終結果の確認
grep 最終結果: log/*.md
```

### プロンプト・スキーマファイルの生成

```bash
# settings/ディレクトリにファイルを生成
uv run eval-dump.py

# 特定のファイルを対象にする場合
uv run eval-dump.py -f your_text.txt
```

## 期待される結果

- **実験1**: 平均スコア4.0以上（文章の内容を評価）
- **実験2-5**: 平均スコア1.00（評価基準と無関係なため）

スコアが期待値から逸脱する場合、モデルが指示を誤解または無視したことを示します。

## 主な知見

実験結果から、以下の指示形式が最も安定していることが判明しました：

1. **フィールド名指示（実験3）**: キー名を評価基準の全文にする
2. **二重指示（実験4）**: プロンプトとフィールド名の両方で指示
3. **キー対応指示（実験5）**: プロンプトでキーと指示を対応付け

**重要**: スキーマのdescriptionのみに指示を含めることは避け、プロンプトやフィールド名で直接的に指示することが推奨されます。

## 必要な依存関係

- Python 3.10+
- uv（パッケージマネージャー）
- llm7shi.compat（LLM統合ライブラリ）
